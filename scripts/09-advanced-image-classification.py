"""
é«˜çº§ PyTorch å›¾åƒåˆ†ç±»æ¼”ç¤º
åŒ…å«å¤šæ¨¡å‹æ¯”è¾ƒã€æ€§èƒ½è¯„ä¼°å’Œå®é™…åº”ç”¨ç¤ºä¾‹
"""

import torch
import torch.nn.functional as F
from torchvision import transforms, models
import time
import json
import random
from datetime import datetime

print("ğŸš€ é«˜çº§ PyTorch å›¾åƒåˆ†ç±»æ¼”ç¤º")
print("=" * 60)

# 1. å¤šæ¨¡å‹æ¶æ„æ¯”è¾ƒ
print("ğŸ—ï¸ æ”¯æŒçš„æ¨¡å‹æ¶æ„")
print("-" * 40)

model_configs = {
    "resnet18": {
        "name": "ResNet-18",
        "params": "11.7M",
        "size": "45MB",
        "accuracy": "69.8%",
        "speed": "50ms",
        "description": "è½»é‡çº§ï¼Œé€‚åˆå¿«é€Ÿæ¨ç†",
    },
    "resnet50": {
        "name": "ResNet-50",
        "params": "25.6M",
        "size": "98MB",
        "accuracy": "76.1%",
        "speed": "120ms",
        "description": "å¹³è¡¡æ€§èƒ½ä¸ç²¾åº¦",
    },
    "mobilenet_v2": {
        "name": "MobileNet V2",
        "params": "3.5M",
        "size": "14MB",
        "accuracy": "71.9%",
        "speed": "30ms",
        "description": "ç§»åŠ¨ç«¯ä¼˜åŒ–ï¼Œæè‡´è½»é‡",
    },
    "efficientnet_b0": {
        "name": "EfficientNet-B0",
        "params": "5.3M",
        "size": "21MB",
        "accuracy": "77.7%",
        "speed": "80ms",
        "description": "æ•ˆç‡ä¼˜åŒ–ï¼ŒSOTAæ€§èƒ½",
    },
}

for model_key, config in model_configs.items():
    print(f"ğŸ“± {config['name']}")
    print(f"   å‚æ•°é‡: {config['params']}")
    print(f"   æ¨¡å‹å¤§å°: {config['size']}")
    print(f"   å‡†ç¡®ç‡: {config['accuracy']}")
    print(f"   æ¨ç†é€Ÿåº¦: {config['speed']}")
    print(f"   ç‰¹ç‚¹: {config['description']}")
    print()

# 2. é«˜çº§é¢„å¤„ç†ç®¡é“
print("ğŸ”§ é«˜çº§å›¾åƒé¢„å¤„ç†ç®¡é“")
print("-" * 40)

# å®šä¹‰å¤šç§é¢„å¤„ç†ç­–ç•¥
preprocessing_strategies = {
    "standard": transforms.Compose(
        [
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ]
    ),
    "augmented": transforms.Compose(
        [
            transforms.Resize(256),
            transforms.RandomCrop(224),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.ColorJitter(brightness=0.2, contrast=0.2),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ]
    ),
    "test_time_augmentation": transforms.Compose(
        [
            transforms.Resize(256),
            transforms.FiveCrop(224),
            transforms.Lambda(
                lambda crops: torch.stack(
                    [transforms.ToTensor()(crop) for crop in crops]
                )
            ),
            transforms.Lambda(
                lambda tensors: torch.stack(
                    [
                        transforms.Normalize(
                            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]
                        )(t)
                        for t in tensors
                    ]
                )
            ),
        ]
    ),
}

print("âœ… æ ‡å‡†é¢„å¤„ç†: åŸºç¡€çš„ç¼©æ”¾å’Œå½’ä¸€åŒ–")
print("âœ… æ•°æ®å¢å¼º: éšæœºè£å‰ªã€ç¿»è½¬ã€é¢œè‰²å˜æ¢")
print("âœ… æµ‹è¯•æ—¶å¢å¼º: å¤šè§†è§’é¢„æµ‹æå‡å‡†ç¡®ç‡")


# 3. æ™ºèƒ½åˆ†ç±»ç³»ç»Ÿ
class AdvancedImageClassifier:
    def __init__(self, model_name="resnet18"):
        self.model_name = model_name
        self.model_config = model_configs.get(model_name, model_configs["resnet18"])
        self.classes = self._load_classes()
        self.preprocessing = preprocessing_strategies["standard"]

    def _load_classes(self):
        """åŠ è½½ç±»åˆ«æ ‡ç­¾"""
        # æ¨¡æ‹Ÿ ImageNet 1000 ç±»åˆ«
        categories = [
            "åŠ¨ç‰©ç±»",
            "æ¤ç‰©ç±»",
            "äº¤é€šå·¥å…·",
            "å»ºç­‘ç‰©",
            "é£Ÿç‰©",
            "å®¶å…·",
            "ç”µå­è®¾å¤‡",
            "è¿åŠ¨å™¨æ",
            "ä¹å™¨",
            "å·¥å…·",
            "æœè£…",
            "ç©å…·",
            "ä¹¦ç±",
            "è‰ºæœ¯å“",
            "è‡ªç„¶æ™¯è§‚",
        ]

        classes = []
        for category in categories:
            for i in range(67):  # æ¯ç±»çº¦67ä¸ªå­ç±»ï¼Œæ€»è®¡çº¦1000ç±»
                classes.append(f"{category}_{i+1}")

        return classes[:1000]

    def predict(self, image_info, top_k=5, use_tta=False):
        """
        æ‰§è¡Œå›¾åƒåˆ†ç±»é¢„æµ‹

        Args:
            image_info: å›¾åƒä¿¡æ¯
            top_k: è¿”å›å‰kä¸ªç»“æœ
            use_tta: æ˜¯å¦ä½¿ç”¨æµ‹è¯•æ—¶å¢å¼º
        """
        start_time = time.time()

        print(f"ğŸ” ä½¿ç”¨ {self.model_config['name']} è¿›è¡Œåˆ†ç±»...")

        # æ¨¡æ‹Ÿé¢„æµ‹è¿‡ç¨‹
        if use_tta:
            print("ğŸ”„ åº”ç”¨æµ‹è¯•æ—¶å¢å¼º (TTA)...")
            time.sleep(0.2)  # TTAéœ€è¦æ›´å¤šæ—¶é—´

        # æ¨¡æ‹Ÿæ¨ç†æ—¶é—´
        inference_time = float(self.model_config["speed"].replace("ms", "")) / 1000
        time.sleep(inference_time)

        # ç”Ÿæˆé¢„æµ‹ç»“æœ
        predictions = self._generate_predictions(image_info, top_k, use_tta)

        total_time = time.time() - start_time

        return {
            "predictions": predictions,
            "model": self.model_config["name"],
            "inference_time": total_time,
            "use_tta": use_tta,
        }

    def _generate_predictions(self, image_info, top_k, use_tta):
        """ç”Ÿæˆæ¨¡æ‹Ÿé¢„æµ‹ç»“æœ"""
        # æ ¹æ®å›¾åƒç±»å‹ç”Ÿæˆç›¸å…³çš„é¢„æµ‹
        image_type = image_info.get("type", "unknown")

        # å®šä¹‰ç±»å‹ç›¸å…³çš„ç±»åˆ«
        type_mapping = {
            "åŠ¨ç‰©": ["çŒ«ç§‘åŠ¨ç‰©", "çŠ¬ç§‘åŠ¨ç‰©", "é¸Ÿç±»", "å“ºä¹³åŠ¨ç‰©", "å® ç‰©"],
            "äº¤é€šå·¥å…·": ["æ±½è½¦", "é£æœº", "èˆ¹åª", "ç«è½¦", "æ‘©æ‰˜è½¦"],
            "æ¤ç‰©": ["èŠ±æœµ", "æ ‘æœ¨", "è‰æœ¬æ¤ç‰©", "è•¨ç±»", "è‹”è—“"],
            "å»ºç­‘": ["ä½å®…", "å•†ä¸šå»ºç­‘", "å¤å»ºç­‘", "ç°ä»£å»ºç­‘", "æ¡¥æ¢"],
            "é£Ÿç‰©": ["æ°´æœ", "è”¬èœ", "è‚‰ç±»", "ç”œç‚¹", "é¥®æ–™"],
        }

        # é€‰æ‹©ç›¸å…³ç±»åˆ«
        relevant_classes = type_mapping.get(image_type, random.sample(self.classes, 20))

        # ç”Ÿæˆç½®ä¿¡åº¦åˆ†æ•°
        base_scores = [random.uniform(0.3, 0.9) for _ in range(top_k)]

        # TTAé€šå¸¸èƒ½æå‡ç½®ä¿¡åº¦
        if use_tta:
            base_scores = [min(0.95, score * 1.1) for score in base_scores]

        base_scores.sort(reverse=True)

        predictions = []
        selected_classes = random.sample(
            relevant_classes, min(top_k, len(relevant_classes))
        )

        for i, (score, class_name) in enumerate(zip(base_scores, selected_classes)):
            predictions.append(
                {
                    "rank": i + 1,
                    "class": class_name,
                    "confidence": score,
                    "percentage": score * 100,
                }
            )

        return predictions


# 4. æ‰¹é‡æµ‹è¯•ä¸åŒæ¨¡å‹
print("\nğŸ§ª å¤šæ¨¡å‹æ€§èƒ½æ¯”è¾ƒ")
print("-" * 40)

test_images = [
    {"name": "é‡‘æ¯›çŠ¬", "type": "åŠ¨ç‰©", "complexity": "ç®€å•"},
    {"name": "è·‘è½¦", "type": "äº¤é€šå·¥å…·", "complexity": "ä¸­ç­‰"},
    {"name": "ç«ç‘°èŠ±", "type": "æ¤ç‰©", "complexity": "ç®€å•"},
    {"name": "æ‘©å¤©å¤§æ¥¼", "type": "å»ºç­‘", "complexity": "å¤æ‚"},
    {"name": "æ„å¤§åˆ©é¢", "type": "é£Ÿç‰©", "complexity": "ä¸­ç­‰"},
]

# æµ‹è¯•æ¯ä¸ªæ¨¡å‹
results_summary = {}

for model_name in ["resnet18", "resnet50", "mobilenet_v2"]:
    print(f"\nğŸ”¬ æµ‹è¯•æ¨¡å‹: {model_configs[model_name]['name']}")
    print("-" * 30)

    classifier = AdvancedImageClassifier(model_name)
    model_results = []

    for img in test_images:
        print(f"\nğŸ“¸ å›¾åƒ: {img['name']} ({img['complexity']})")

        # æ ‡å‡†é¢„æµ‹
        result = classifier.predict(img, top_k=3, use_tta=False)
        print(f"   â±ï¸ æ¨ç†æ—¶é—´: {result['inference_time']:.3f}s")
        print("   ğŸ¯ é¢„æµ‹ç»“æœ:")

        for pred in result["predictions"]:
            print(f"      {pred['rank']}. {pred['class']}: {pred['percentage']:.1f}%")

        model_results.append(result)

    # è®¡ç®—æ¨¡å‹å¹³å‡æ€§èƒ½
    avg_time = sum(r["inference_time"] for r in model_results) / len(model_results)
    avg_confidence = sum(
        r["predictions"][0]["confidence"] for r in model_results
    ) / len(model_results)

    results_summary[model_name] = {
        "avg_time": avg_time,
        "avg_confidence": avg_confidence,
        "model_config": classifier.model_config,
    }

    print(f"\nğŸ“Š {classifier.model_config['name']} æ€§èƒ½æ€»ç»“:")
    print(f"   å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.3f}s")
    print(f"   å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.1f}%")

# 5. æ€§èƒ½å¯¹æ¯”åˆ†æ
print("\nğŸ“ˆ æ¨¡å‹æ€§èƒ½å¯¹æ¯”åˆ†æ")
print("=" * 50)

print("æ¨¡å‹åç§°          æ¨ç†æ—¶é—´    ç½®ä¿¡åº¦    å‚æ•°é‡    æ¨¡å‹å¤§å°")
print("-" * 55)

for model_name, results in results_summary.items():
    config = results["model_config"]
    print(
        f"{config['name']:<15} {results['avg_time']:.3f}s     {results['avg_confidence']:.1f}%     {config['params']:<8} {config['size']}"
    )

# 6. å®é™…åº”ç”¨åœºæ™¯æ¼”ç¤º
print("\nğŸ¯ å®é™…åº”ç”¨åœºæ™¯æ¼”ç¤º")
print("-" * 40)

application_scenarios = [
    {
        "name": "ç§»åŠ¨ç«¯å®æ—¶åˆ†ç±»",
        "model": "mobilenet_v2",
        "requirements": ["ä½å»¶è¿Ÿ", "å°æ¨¡å‹", "çœç”µ"],
        "use_case": "æ‰‹æœºç›¸å†Œè‡ªåŠ¨åˆ†ç±»",
    },
    {
        "name": "æœåŠ¡å™¨ç«¯æ‰¹å¤„ç†",
        "model": "resnet50",
        "requirements": ["é«˜ç²¾åº¦", "æ‰¹å¤„ç†", "ç¨³å®šæ€§"],
        "use_case": "ç”µå•†å•†å“è‡ªåŠ¨æ ‡æ³¨",
    },
    {
        "name": "è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²",
        "model": "resnet18",
        "requirements": ["å¹³è¡¡æ€§èƒ½", "é€‚ä¸­å¤§å°", "å¿«é€Ÿå“åº”"],
        "use_case": "æ™ºèƒ½ç›‘æ§ç³»ç»Ÿ",
    },
]

for scenario in application_scenarios:
    print(f"\nğŸš€ {scenario['name']}")
    print(f"   æ¨èæ¨¡å‹: {model_configs[scenario['model']]['name']}")
    print(f"   å…³é”®éœ€æ±‚: {', '.join(scenario['requirements'])}")
    print(f"   åº”ç”¨æ¡ˆä¾‹: {scenario['use_case']}")

# 7. é«˜çº§ä¼˜åŒ–æŠ€æœ¯
print("\nâš¡ é«˜çº§ä¼˜åŒ–æŠ€æœ¯")
print("-" * 40)

optimization_techniques = [
    {
        "name": "æ¨¡å‹é‡åŒ– (Quantization)",
        "benefit": "æ¨¡å‹å¤§å°å‡å°‘75%ï¼Œæ¨ç†é€Ÿåº¦æå‡2-4å€",
        "trade_off": "ç²¾åº¦è½»å¾®ä¸‹é™(1-2%)",
    },
    {
        "name": "æ¨¡å‹å‰ªæ (Pruning)",
        "benefit": "ç§»é™¤å†—ä½™å‚æ•°ï¼Œå‡å°‘è®¡ç®—é‡",
        "trade_off": "éœ€è¦é‡æ–°è®­ç»ƒå¾®è°ƒ",
    },
    {
        "name": "çŸ¥è¯†è’¸é¦ (Knowledge Distillation)",
        "benefit": "å°æ¨¡å‹å­¦ä¹ å¤§æ¨¡å‹çŸ¥è¯†",
        "trade_off": "è®­ç»ƒè¿‡ç¨‹æ›´å¤æ‚",
    },
    {
        "name": "åŠ¨æ€æ¨ç† (Dynamic Inference)",
        "benefit": "æ ¹æ®è¾“å…¥å¤æ‚åº¦è°ƒæ•´è®¡ç®—",
        "trade_off": "å®ç°å¤æ‚åº¦è¾ƒé«˜",
    },
]

for tech in optimization_techniques:
    print(f"\nğŸ”§ {tech['name']}")
    print(f"   ä¼˜åŠ¿: {tech['benefit']}")
    print(f"   æƒè¡¡: {tech['trade_off']}")

# 8. éƒ¨ç½²å»ºè®®
print("\nğŸš€ éƒ¨ç½²å»ºè®®")
print("-" * 40)

deployment_options = {
    "äº‘ç«¯éƒ¨ç½²": {
        "platforms": ["AWS SageMaker", "Google Cloud AI", "Azure ML"],
        "advantages": ["é«˜æ€§èƒ½GPU", "å¼¹æ€§æ‰©å±•", "æ‰˜ç®¡æœåŠ¡"],
        "considerations": ["ç½‘ç»œå»¶è¿Ÿ", "æ•°æ®ä¼ è¾“æˆæœ¬"],
    },
    "è¾¹ç¼˜éƒ¨ç½²": {
        "platforms": ["NVIDIA Jetson", "Intel NCS", "Google Coral"],
        "advantages": ["ä½å»¶è¿Ÿ", "æ•°æ®éšç§", "ç¦»çº¿å·¥ä½œ"],
        "considerations": ["ç¡¬ä»¶é™åˆ¶", "æ¨¡å‹ä¼˜åŒ–éœ€æ±‚"],
    },
    "ç§»åŠ¨ç«¯éƒ¨ç½²": {
        "platforms": ["Core ML (iOS)", "TensorFlow Lite", "ONNX Runtime"],
        "advantages": ["ç”¨æˆ·ä½“éªŒå¥½", "æ— ç½‘ç»œä¾èµ–", "å®æ—¶å“åº”"],
        "considerations": ["æ¨¡å‹å¤§å°é™åˆ¶", "ç”µæ± æ¶ˆè€—"],
    },
}

for deployment, details in deployment_options.items():
    print(f"\nğŸ“± {deployment}")
    print(f"   å¹³å°: {', '.join(details['platforms'])}")
    print(f"   ä¼˜åŠ¿: {', '.join(details['advantages'])}")
    print(f"   è€ƒè™‘å› ç´ : {', '.join(details['considerations'])}")

# 9. ç›‘æ§å’Œç»´æŠ¤
print("\nğŸ“Š ç”Ÿäº§ç¯å¢ƒç›‘æ§")
print("-" * 40)

monitoring_metrics = [
    "ğŸ¯ æ¨¡å‹å‡†ç¡®ç‡ç›‘æ§",
    "â±ï¸ æ¨ç†å»¶è¿Ÿè·Ÿè¸ª",
    "ğŸ’¾ å†…å­˜ä½¿ç”¨ç›‘æ§",
    "ğŸ”„ è¯·æ±‚ååé‡ç»Ÿè®¡",
    "âŒ é”™è¯¯ç‡åˆ†æ",
    "ğŸ“ˆ æ•°æ®æ¼‚ç§»æ£€æµ‹",
    "ğŸ”§ æ¨¡å‹ç‰ˆæœ¬ç®¡ç†",
    "ğŸ“ é¢„æµ‹ç»“æœå®¡è®¡",
]

for metric in monitoring_metrics:
    print(f"   {metric}")

# 10. æ€»ç»“å’Œæœ€ä½³å®è·µ
print("\nâœ¨ æœ€ä½³å®è·µæ€»ç»“")
print("=" * 50)

best_practices = [
    "ğŸ¯ æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ¨¡å‹æ¶æ„",
    "ğŸ“Š åœ¨çœŸå®æ•°æ®ä¸ŠéªŒè¯æ¨¡å‹æ€§èƒ½",
    "âš¡ ä½¿ç”¨é€‚å½“çš„ä¼˜åŒ–æŠ€æœ¯æå‡æ•ˆç‡",
    "ğŸ”„ å»ºç«‹å®Œå–„çš„æ¨¡å‹æ›´æ–°æµç¨‹",
    "ğŸ“ˆ æŒç»­ç›‘æ§ç”Ÿäº§ç¯å¢ƒæ€§èƒ½",
    "ğŸ›¡ï¸ å®æ–½æ¨¡å‹å®‰å…¨å’Œéšç§ä¿æŠ¤",
    "ğŸ“ ç»´æŠ¤è¯¦ç»†çš„å®éªŒå’Œéƒ¨ç½²æ–‡æ¡£",
    "ğŸ§ª å®šæœŸè¿›è¡ŒA/Bæµ‹è¯•ä¼˜åŒ–ç”¨æˆ·ä½“éªŒ",
]

for practice in best_practices:
    print(f"   {practice}")

print(f"\nğŸ‰ é«˜çº§å›¾åƒåˆ†ç±»æ¼”ç¤ºå®Œæˆï¼")
print(f"â° æ¼”ç¤ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("ğŸš€ æ‚¨ç°åœ¨å¯ä»¥æ„å»ºä¼ä¸šçº§çš„å›¾åƒåˆ†ç±»ç³»ç»Ÿäº†ï¼")
